{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "graphic-playing",
   "metadata": {},
   "source": [
    "# LAB 3: Automated Terminology Extraction\n",
    "\n",
    "Extract technical terms from ACL Anthology\n",
    "\n",
    "Objectives:\n",
    "* part of speech tagging with spacy\n",
    "* extract phrases that match a part of speech pattern\n",
    "* scale processing pipeline with dask\n",
    "* compute c-values\n",
    "\n",
    "## Part I: Test c-value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "durable-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cytoolz import *\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "respiratory-florence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:32919</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>16.62 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:32919' processes=4 threads=4, memory=16.62 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://127.0.0.1:32919\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sixth-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('s3://ling583/acl.parquet', storage_options={'anon':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "descending-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "overall-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.from_pandas(df, npartitions=100)\n",
    "texts = df['text'].to_bag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-pride",
   "metadata": {},
   "source": [
    "### Set up spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mexican-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "light-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_core_web_sm is an english model built on data from the web\n",
    "# the \"sm\" denotes small model, there are larger models available but we don't need all of that\n",
    "## Excluded modules:\n",
    "# Parser finds the syntactic structure of sentences\n",
    "# ner (Named Entity Recognizer) pulls out names of people and places\n",
    "# lemmatizer strips imflection and morphology from words to find their root\n",
    "# attribute_ruler identifies gender of pronouns and more\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['parser', 'ner', 'lemmatizer', 'attribute_ruler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "naked-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "flush-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matcher and link it to our vocabulary established above\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# add rules, in this case define candidate terms\n",
    "# IN just means in the set: []\n",
    "# JJ = adjective\n",
    "# NN = noun\n",
    "# IN = preposition\n",
    "# HYPH = hyphen\n",
    "# OP = operation, works like regular expressions (* = zero or more times)\n",
    "matcher.add('Term', [[{'TAG': {'IN': ['JJ', 'NN']}},\n",
    "                      {'TAG': {'IN': ['JJ', 'NN', 'IN', 'HYPH']}, 'OP': '*'},\n",
    "                      {'TAG': 'NN'}]])\n",
    "# this amounts to any noun/adjective followed by and number of adjective/noun/preposition/hyphen, ending with another noun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-republican",
   "metadata": {},
   "source": [
    "### Extract candidate terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "blond-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(text):\n",
    "    doc = nlp(text) # tokenize and tag\n",
    "    spans = matcher(doc, as_spans=True) # find all of the spans that satisfy the rules above\n",
    "    return [tuple(tok.norm_ for tok in span) for span in spans] # return a list of all of the spans converted to tuples of normalized strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "soviet-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \\ concatenates the lines \n",
    "graph = texts.map(get_candidates) \\\n",
    "             .flatten() \\\n",
    "             .frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "proved-david",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 1.65 s, total: 12.6 s\n",
      "Wall time: 5min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "candidates = graph.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "warming-somerset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('polynomial', 'time'), 234),\n",
       " (('recognition', 'phase'), 17),\n",
       " (('input', 'string'), 379),\n",
       " (('spurious', 'ambiguity'), 148),\n",
       " (('function', 'application'), 40),\n",
       " (('relative', 'ordering'), 29),\n",
       " (('considerable', 'interest'), 40),\n",
       " (('large', 'number'), 1357),\n",
       " (('same', 'function'), 26),\n",
       " (('function', 'argument'), 5)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-marks",
   "metadata": {},
   "source": [
    "### Compute c-values\n",
    "\n",
    "$$\\mbox{C-value}(a)=\\begin{cases}\\log_2|a|\\cdot f(a) & \\mbox{if } a \\mbox{ is not nested}\\\\\\log_2|a|\\left(f(a)-\\frac{1}{P(T_a)}\\sum_{b\\in T_a}f(b)\\right) & \\mbox{otherwise}\\\\\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "quiet-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "confused-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict\n",
    "# Keys = sequence lengths\n",
    "# Values = counter of sequences of that length\n",
    "freqs = defaultdict(Counter)\n",
    "for c, f in candidates:\n",
    "    freqs[len(c)][c] =f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "desperate-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "clear-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of the term, then create a list 1 smaller than that length that decreases until 2\n",
    "def get_subterms(term):\n",
    "    k = len(term)\n",
    "    for m in range(k-1, 1, -1):\n",
    "        yield from ngrams(term, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "alternative-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "korean-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F = frequency data structure defined above, sorted by length\n",
    "# theta = Threshold, the C-value above which we consider candidates to be terms\n",
    "def c_value(F, theta):\n",
    "    \n",
    "    # Keep track of terms as we identify them\n",
    "    termhood = Counter()\n",
    "    \n",
    "    # Keep track of longer sequences that contain shorter sequences\n",
    "    longer = defaultdict(list)\n",
    "    \n",
    "    # K is sequence length, starting with the longest\n",
    "    for k in sorted(F, reverse=True):\n",
    "        for term in F[k]:\n",
    "            # if the term is a subsequence of a longer one that we have seen already\n",
    "            if term in longer:\n",
    "                discount = sum(longer[term]) / len(longer[term])\n",
    "            # if there are no longer sequences of it, there is no discount\n",
    "            else:\n",
    "                discount = 0\n",
    "            c = log2(k) * (F[k][term] - discount)\n",
    "            if c > theta:\n",
    "                termhood[term] = c\n",
    "                for subterm in get_subterms(term):\n",
    "                    if subterm in F[len(subterm)]:\n",
    "                        longer[subterm].append(F[k][term])\n",
    "    return termhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "pointed-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = c_value(freqs, theta=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "sexual-westminster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5236.00 5682 language model\n",
      " 4461.67 5060 machine translation\n",
      " 4451.14 2330 part - of - speech\n",
      " 4410.50 5388 natural language\n",
      " 3583.00 3583 training set\n",
      " 3379.00 3920 neural network\n",
      " 3346.00 3346 previous work\n",
      " 3171.75 1366 end - to - end\n",
      " 3012.00 3012 other hand\n",
      " 3003.00 3003 test set\n",
      " 2923.00 2923 future work\n",
      " 2589.83 1634 natural language processing\n",
      " 2370.00 2370 target language\n",
      " 2317.22 1462 sentence - level\n",
      " 2301.37 1452 large - scale\n",
      " 2209.44 1394 word - level\n",
      " 2174.00 2174 parse tree\n",
      " 2144.45 1353 n - gram\n",
      " 2059.00 2059 training corpus\n",
      " 2019.24 1274 f - score\n"
     ]
    }
   ],
   "source": [
    "for t, c in terms.most_common(20):\n",
    "    print(f'{c:8.2f} {freqs[len(t)][t]:4d} {\" \".join(t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "removable-donna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  516.70  326 sub - word\n",
      "  515.11  325 chinese word segmentation\n",
      "  515.00  515 lexical information\n",
      "  515.00  515 morphological analysis\n",
      "  515.00  515 input sequence\n",
      "  514.00  514 classification problem\n",
      "  513.00  513 local context\n",
      "  512.00  512 time complexity\n",
      "  511.00  511 text generation\n",
      "  511.00  511 probabilistic model\n",
      "  511.00  511 tree kernel\n",
      "  510.00  510 phrase pair\n",
      "  509.00  509 distributional similarity\n",
      "  508.77  321 natural language generation\n",
      "  508.77  321 f1 - score\n",
      "  508.77  321 hyper - parameter\n",
      "  507.19  320 set of candidate\n",
      "  507.00  507 standard deviation\n",
      "  504.00  504 beam size\n",
      "  503.00  503 dependency relation\n"
     ]
    }
   ],
   "source": [
    "for t, c in tail(20, terms.most_common()):\n",
    "    print(f'{c:8.2f} {freqs[len(t)][t]:4d} {\" \".join(t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "subtle-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('terms.txt', 'w') as f:\n",
    "    for term in terms:\n",
    "        print(' '.join(term), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-impossible",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
