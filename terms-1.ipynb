{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "structural-knight",
   "metadata": {},
   "source": [
    "# LAB 3: Automated Terminology Extraction\n",
    "\n",
    "Extract technical terms from ACL Anthology\n",
    "\n",
    "Objectives:\n",
    "* part of speech tagging with spacy\n",
    "* extract phrases that match a part of speech pattern\n",
    "* scale processing pipeline with dask\n",
    "* compute c-values\n",
    "\n",
    "## Part I: Test c-value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "married-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cytoolz import *\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polyphonic-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('s3://ling583/acl.parquet', storage_options={'anon':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "whole-aberdeen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990</td>\n",
       "      <td>P90-1001</td>\n",
       "      <td>Polynomial Time Parsing of Combinatory Categor...</td>\n",
       "      <td>Polynomial Time Parsing of Combinatory Categor...</td>\n",
       "      <td>In this paper we present a polynomial time par...</td>\n",
       "      <td>Combinatory Categorial Grammar (CCG)  is an ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990</td>\n",
       "      <td>P90-1002</td>\n",
       "      <td>Structure and Intonation in Spoken Language Un...</td>\n",
       "      <td>Structure and Intonation in Spoken Language Un...</td>\n",
       "      <td>The structure imposed upon spoken sentences by...</td>\n",
       "      <td>Halliday  observed that this constraint, which...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990</td>\n",
       "      <td>P90-1003</td>\n",
       "      <td>Prosody, Syntax and Parsing</td>\n",
       "      <td>Prosody, Syntax and Parsing We describe the mo...</td>\n",
       "      <td>We describe the modification of a grammar to t...</td>\n",
       "      <td>Prosodic information can mark lexical stress, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990</td>\n",
       "      <td>P90-1004</td>\n",
       "      <td>Empirical Study of Predictive Powers of Simple...</td>\n",
       "      <td>Empirical Study of Predictive Powers of Simple...</td>\n",
       "      <td>This empirical study attempts to find answers ...</td>\n",
       "      <td>Difficulty in resolving structural ambiguity i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990</td>\n",
       "      <td>P90-1005</td>\n",
       "      <td>Structural Disambiguation With Constraint Prop...</td>\n",
       "      <td>Structural Disambiguation With Constraint Prop...</td>\n",
       "      <td>We present a new grammatical formalism called ...</td>\n",
       "      <td>We are interested in an efficient treatment of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year       tag                                              title  \\\n",
       "0  1990  P90-1001  Polynomial Time Parsing of Combinatory Categor...   \n",
       "1  1990  P90-1002  Structure and Intonation in Spoken Language Un...   \n",
       "2  1990  P90-1003                        Prosody, Syntax and Parsing   \n",
       "3  1990  P90-1004  Empirical Study of Predictive Powers of Simple...   \n",
       "4  1990  P90-1005  Structural Disambiguation With Constraint Prop...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Polynomial Time Parsing of Combinatory Categor...   \n",
       "1  Structure and Intonation in Spoken Language Un...   \n",
       "2  Prosody, Syntax and Parsing We describe the mo...   \n",
       "3  Empirical Study of Predictive Powers of Simple...   \n",
       "4  Structural Disambiguation With Constraint Prop...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  In this paper we present a polynomial time par...   \n",
       "1  The structure imposed upon spoken sentences by...   \n",
       "2  We describe the modification of a grammar to t...   \n",
       "3  This empirical study attempts to find answers ...   \n",
       "4  We present a new grammatical formalism called ...   \n",
       "\n",
       "                                                body  \n",
       "0  Combinatory Categorial Grammar (CCG)  is an ex...  \n",
       "1  Halliday  observed that this constraint, which...  \n",
       "2  Prosodic information can mark lexical stress, ...  \n",
       "3  Difficulty in resolving structural ambiguity i...  \n",
       "4  We are interested in an efficient treatment of...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "foster-chick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6167"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 6167 articles. This is more than we need for this part\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "curious-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cut the dataframe down to a smaller size\n",
    "# Note: Each time this is run, if the random state is the same, \n",
    "# we will receive the same \"random\" samples\n",
    "df = df.sample(500, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "understanding-treatment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-ontario",
   "metadata": {},
   "source": [
    "### Set up spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beginning-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "serial-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_core_web_sm is an english model built on data from the web\n",
    "# the \"sm\" denotes small model, there are larger models available but we don't need all of that\n",
    "## Excluded modules:\n",
    "# Parser finds the syntactic structure of sentences\n",
    "# ner (Named Entity Recognizer) pulls out names of people and places\n",
    "# lemmatizer strips imflection and morphology from words to find their root\n",
    "# attribute_ruler identifies gender of pronouns and more\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['parser', 'ner', 'lemmatizer', 'attribute_ruler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "above-procurement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f37efac4360>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f37efae22c0>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the modules that we are left with after exclusions\n",
    "# took2vec (token to vector) looks words up in the vocabulary\n",
    "# tagger tags the part of speech\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "composite-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our text through the pipeline\n",
    "doc = nlp(df['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at what the results are\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "human-fairy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resume"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can loook at the data token by token\n",
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "muslim-company",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resume Information Extraction with Cascaded Hybrid Model This paper presents"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can also look at the data in spans\n",
    "doc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "occupied-calculation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NNP', 'resume')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .tag_ part of speech label\n",
    "# .norm_ normalized form\n",
    "doc[0].tag_, doc[0].norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stunning-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "phantom-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matcher and link it to our vocabulary established above\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# add rules, in this case define candidate terms\n",
    "# IN just means in the set: []\n",
    "# JJ = adjective\n",
    "# NN = noun\n",
    "# IN = preposition\n",
    "# HYPH = hyphen\n",
    "# OP = operation, works like regular expressions (* = zero or more times)\n",
    "matcher.add('Term', [[{'TAG': {'IN': ['JJ', 'NN']}},\n",
    "                      {'TAG': {'IN': ['JJ', 'NN', 'IN', 'HYPH']}, 'OP': '*'},\n",
    "                      {'TAG': 'NN'}]])\n",
    "# this amounts to any noun/adjective followed by and number of adjective/noun/preposition/hyphen, ending with another noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cleared-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = matcher(doc, as_spans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "military-happening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('effective', 'approach')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(tok.norm_ for tok in spans[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-freeware",
   "metadata": {},
   "source": [
    "### Extract candidate terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "accepted-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(text):\n",
    "    doc = nlp(text) # tokenize and tag\n",
    "    spans = matcher(doc, as_spans=True) # find all of the spans that satisfy the rules above\n",
    "    return [tuple(tok.norm_ for tok in span) for span in spans] # return a list of all of the spans converted to tuples of normalized strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the function on the first article in the database\n",
    "get_candidates(df['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "injured-parish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0530788279d744f2a6f280950f443c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the process on the entire database\n",
    "candidates = list(concat(df['text'].progress_apply(get_candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-wrist",
   "metadata": {},
   "source": [
    "### Compute c-values\n",
    "\n",
    "$$\\mbox{C-value}(a)=\\begin{cases}\\log_2|a|\\cdot f(a) & \\mbox{if } a \\mbox{ is not nested}\\\\\\log_2|a|\\left(f(a)-\\frac{1}{P(T_a)}\\sum_{b\\in T_a}f(b)\\right) & \\mbox{otherwise}\\\\\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "lined-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "hungarian-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict\n",
    "# Keys = sequence lengths\n",
    "# Values = counter of sequences of that length\n",
    "freqs = defaultdict(Counter)\n",
    "for c in candidates:\n",
    "    freqs[len(c)][c] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "narrow-album",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the longest sequence is 32 characters long\n",
    "freqs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "raising-responsibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('p',\n",
       "          'conj',\n",
       "          'p',\n",
       "          '#',\n",
       "          'w',\n",
       "          'p',\n",
       "          'p',\n",
       "          'exp',\n",
       "          'c',\n",
       "          'p',\n",
       "          'conj',\n",
       "          'p',\n",
       "          '#',\n",
       "          'w',\n",
       "          'p',\n",
       "          'p',\n",
       "          'exp',\n",
       "          'h',\n",
       "          'conj',\n",
       "          'p',\n",
       "          '#',\n",
       "          'c',\n",
       "          'p',\n",
       "          '#',\n",
       "          'w',\n",
       "          'p',\n",
       "          's#h',\n",
       "          'exp',\n",
       "          'p',\n",
       "          'p',\n",
       "          'top',\n",
       "          'c'): 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this one is an equation that the pdf to text converter did something weird to\n",
    "freqs[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cardiac-appraisal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('part', '-', 'of', '-', 'speech'), 213),\n",
       " (('end', '-', 'to', '-', 'end'), 99),\n",
       " (('tree', '-', 'to', '-', 'string'), 57),\n",
       " (('sequence', '-', 'to', '-', 'sequence'), 41),\n",
       " (('state', '-', 'ofthe', '-', 'art'), 37)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs[5].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "round-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "synthetic-resistance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 2]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we are wanting to look at the sequences of length 5 or less\n",
    "# start with N-1 (5-1) and count down from there\n",
    "list(range(4, 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "gross-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of the term, then create a list 1 smaller than that length that decreases until 2\n",
    "def get_subterms(term):\n",
    "    k = len(term)\n",
    "    for m in range(k-1, 1, -1):\n",
    "        yield from ngrams(term, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fitted-administration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('part', '-', 'of', '-'),\n",
       " ('-', 'of', '-', 'speech'),\n",
       " ('part', '-', 'of'),\n",
       " ('-', 'of', '-'),\n",
       " ('of', '-', 'speech'),\n",
       " ('part', '-'),\n",
       " ('-', 'of'),\n",
       " ('of', '-'),\n",
       " ('-', 'speech')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on one of the previously found candidates\n",
    "list(get_subterms(('part', '-', 'of', '-', 'speech')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "freelance-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "horizontal-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F = frequency data structure defined above, sorted by length\n",
    "# theta = Threshold, the C-value above which we consider candidates to be terms\n",
    "def c_value(F, theta):\n",
    "    \n",
    "    # Keep track of terms as we identify them\n",
    "    termhood = Counter()\n",
    "    \n",
    "    # Keep track of longer sequences that contain shorter sequences\n",
    "    longer = defaultdict(list)\n",
    "    \n",
    "    # K is sequence length, starting with the longest\n",
    "    for k in sorted(F, reverse=True):\n",
    "        for term in F[k]:\n",
    "            # if the term is a subsequence of a longer one that we have seen already\n",
    "            if term in longer:\n",
    "                discount = sum(longer[term]) / len(longer[term])\n",
    "            # if there are no longer sequences of it, there is no discount\n",
    "            else:\n",
    "                discount = 0\n",
    "            c = log2(k) * (F[k][term] - discount)\n",
    "            if c > theta:\n",
    "                termhood[term] = c\n",
    "                for subterm in get_subterms(term):\n",
    "                    if subterm in F[len(subterm)]:\n",
    "                        longer[subterm].append(F[k][term])\n",
    "    return termhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "atmospheric-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = c_value(freqs, theta=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "happy-tonight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  446.00  446 language model\n",
      "  420.27  213 part - of - speech\n",
      "  317.00  458 natural language\n",
      "  310.00  310 training set\n",
      "  307.48  194 sentence - level\n",
      "  298.00  381 machine translation\n",
      "  271.00  271 other hand\n",
      "  265.00  265 test set\n",
      "  261.00  261 previous work\n",
      "  251.00  306 neural network\n",
      "  242.50  153 word - level\n",
      "  238.00  238 word alignment\n",
      "  229.87   99 end - to - end\n",
      "  223.48  141 natural language processing\n",
      "  220.00  220 future work\n",
      "  209.22  132 n - gram\n",
      "  209.22  132 large - scale\n",
      "  198.00  270 co -\n",
      "  194.95  123 f - measure\n",
      "  193.37  122 f - score\n"
     ]
    }
   ],
   "source": [
    "for t, c in terms.most_common(20):\n",
    "    print(f'{c:8.2f} {freqs[len(t)][t]:4d} {\" \".join(t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "first-billion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   84.00   84 target domain\n",
      "   83.00   83 sentence level\n",
      "   82.72   32 part - of - speech tagging\n",
      "   80.83   51 part of speech\n",
      "   80.00   80 first step\n",
      "   80.00   80 head word\n",
      "   80.00   80 sentence pair\n",
      "   79.00   79 time step\n",
      "   78.00   78 baseline system\n",
      "   78.00   78 - word\n",
      "   77.66   49 t - test\n",
      "   77.66   49 character - level\n",
      "   77.00   77 related work\n",
      "   77.00   77 sentence compression\n",
      "   76.00   76 mutual information\n",
      "   76.00   76 re -\n",
      "   76.00   76 small number\n",
      "   76.00   76 deep learning\n",
      "   76.00   76 recent work\n",
      "   76.00   76 text classification\n"
     ]
    }
   ],
   "source": [
    "for t, c in tail(20, terms.most_common()):\n",
    "    print(f'{c:8.2f} {freqs[len(t)][t]:4d} {\" \".join(t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-french",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
