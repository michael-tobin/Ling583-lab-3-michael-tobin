{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "logical-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cytoolz import *\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "certified-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [t.split() for t in open('terms.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "other-merchant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['part', '-', 'of', '-', 'speech', 'tagging'],\n",
       " ['word', '-', 'to', '-', 'word'],\n",
       " ['part', '-', 'of', '-', 'speech'],\n",
       " ['state', '-', 'ofthe', '-', 'art'],\n",
       " ['tree', '-', 'to', '-', 'string'],\n",
       " ['-', 'fold', 'cross', '-', 'validation'],\n",
       " ['end', '-', 'to', '-', 'end'],\n",
       " ['state', '-', 'of', '-', 'theart'],\n",
       " ['sequence', '-', 'to', '-', 'sequence'],\n",
       " ['context', '-', 'free', 'grammar'],\n",
       " ['right', '-', 'hand', 'side'],\n",
       " ['log', '-', 'linear', 'model'],\n",
       " ['-', 'fold', 'cross', 'validation'],\n",
       " ['multi', '-', 'document', 'summarization'],\n",
       " ['inter', '-', 'annotator', 'agreement'],\n",
       " ['semi', '-', 'supervised', 'learning'],\n",
       " ['multi', '-', 'task', 'learning'],\n",
       " ['pre', '-', 'trained', 'word'],\n",
       " ['natural', 'language', 'processing'],\n",
       " ['high', '-', 'level'],\n",
       " ['low', '-', 'level'],\n",
       " ['machine', 'translation', 'system'],\n",
       " ['first', '-', 'order'],\n",
       " ['sentence', '-', 'level'],\n",
       " ['predicate', '-', 'argument'],\n",
       " ['natural', 'language', 'generation'],\n",
       " ['point', 'of', 'view'],\n",
       " ['part', 'of', 'speech'],\n",
       " ['finite', '-', 'state'],\n",
       " ['long', '-', 'distance'],\n",
       " ['real', '-', 'time'],\n",
       " ['word', 'sense', 'disambiguation'],\n",
       " ['co', '-', 'occurrence'],\n",
       " ['trade', '-', 'off'],\n",
       " ['large', '-', 'scale'],\n",
       " ['real', '-', 'world'],\n",
       " ['high', '-', 'quality'],\n",
       " ['statistical', 'machine', 'translation'],\n",
       " ['n', '-', 'gram'],\n",
       " ['long', '-', 'term'],\n",
       " ['question', '-', 'answer'],\n",
       " ['set', 'of', 'candidate'],\n",
       " ['number', 'of', 'training'],\n",
       " ['amount', 'of', 'training'],\n",
       " ['second', '-', 'order'],\n",
       " ['word', '-', 'level'],\n",
       " ['f', '-', 'measure'],\n",
       " ['log', '-', 'likelihood'],\n",
       " ['chinese', 'word', 'segmentation'],\n",
       " ['t', '-', 'test'],\n",
       " ['character', '-', 'level'],\n",
       " ['feed', '-', 'forward'],\n",
       " ['document', '-', 'level'],\n",
       " ['gram', 'language', 'model'],\n",
       " ['gold', '-', 'standard'],\n",
       " ['open', '-', 'domain'],\n",
       " ['f', '-', 'score'],\n",
       " ['semantic', 'role', 'labeling'],\n",
       " ['self', '-', 'training'],\n",
       " ['recurrent', 'neural', 'network'],\n",
       " ['stochastic', 'gradient', 'descent'],\n",
       " ['f1', '-', 'score'],\n",
       " ['low', '-', 'resource'],\n",
       " ['hyper', '-', 'parameter'],\n",
       " ['sub', '-', 'word'],\n",
       " ['token', '-', 'level'],\n",
       " ['target', '-', 'side'],\n",
       " ['source', '-', 'side'],\n",
       " ['ground', '-', 'truth'],\n",
       " ['skip', '-', 'gram'],\n",
       " ['neural', 'machine', 'translation'],\n",
       " ['encoder', '-', 'decoder'],\n",
       " ['self', '-', 'attention'],\n",
       " ['large', 'number'],\n",
       " ['same', 'way'],\n",
       " ['time', 'complexity'],\n",
       " ['natural', 'language'],\n",
       " ['previous', 'work'],\n",
       " ['syntactic', 'structure'],\n",
       " ['recent', 'work'],\n",
       " ['logical', 'form'],\n",
       " ['language', 'understanding'],\n",
       " ['speech', 'recognition'],\n",
       " ['related', 'work'],\n",
       " ['standard', 'deviation'],\n",
       " ['test', 'set'],\n",
       " ['semantic', 'information'],\n",
       " ['noun', 'phrase'],\n",
       " ['next', 'section'],\n",
       " ['small', 'number'],\n",
       " ['parse', 'tree'],\n",
       " ['sentence', 'length'],\n",
       " ['other', 'hand'],\n",
       " ['dependency', 'structure'],\n",
       " ['phrase', 'structure'],\n",
       " ['input', 'sentence'],\n",
       " ['search', 'space'],\n",
       " ['co', '-'],\n",
       " ['machine', 'translation'],\n",
       " ['future', 'research'],\n",
       " ['previous', 'section'],\n",
       " ['word', 'sense'],\n",
       " ['same', 'sentence'],\n",
       " ['head', 'word'],\n",
       " ['knowledge', 'base'],\n",
       " ['semantic', 'representation'],\n",
       " ['sub', '-'],\n",
       " ['sentence', 'level'],\n",
       " ['contextual', 'information'],\n",
       " ['large', 'amount'],\n",
       " ['discourse', 'structure'],\n",
       " ['average', 'number'],\n",
       " ['current', 'state'],\n",
       " ['future', 'work'],\n",
       " ['single', 'word'],\n",
       " ['same', 'time'],\n",
       " ['cross', '-'],\n",
       " ['first', 'step'],\n",
       " ['decision', 'tree'],\n",
       " ['root', 'node'],\n",
       " ['target', 'language'],\n",
       " ['source', 'language'],\n",
       " ['wide', 'range'],\n",
       " ['english', 'translation'],\n",
       " ['tree', 'structure'],\n",
       " ['prior', 'knowledge'],\n",
       " ['non', '-'],\n",
       " ['text', 'generation'],\n",
       " ['feature', 'set'],\n",
       " ['e', '-'],\n",
       " ['syntactic', 'information'],\n",
       " ['lexical', 'information'],\n",
       " ['beam', 'search'],\n",
       " ['evaluation', 'metric'],\n",
       " ['total', 'number'],\n",
       " ['time', 'step'],\n",
       " ['generative', 'model'],\n",
       " ['dynamic', 'programming'],\n",
       " ['phrase', 'table'],\n",
       " ['information', 'retrieval'],\n",
       " ['word', 'pair'],\n",
       " ['training', 'corpus'],\n",
       " ['training', 'set'],\n",
       " ['mutual', 'information'],\n",
       " ['similarity', 'measure'],\n",
       " ['error', 'rate'],\n",
       " ['dialog', 'system'],\n",
       " ['morphological', 'analysis'],\n",
       " ['linguistic', 'knowledge'],\n",
       " ['re', '-'],\n",
       " ['semantic', 'role'],\n",
       " ['word', 'order'],\n",
       " ['semantic', 'parsing'],\n",
       " ['neural', 'network'],\n",
       " ['scoring', 'function'],\n",
       " ['training', 'time'],\n",
       " ['target', 'word'],\n",
       " ['language', 'model'],\n",
       " ['word', 'level'],\n",
       " ['source', 'word'],\n",
       " ['source', 'sentence'],\n",
       " ['target', 'sentence'],\n",
       " ['english', 'word'],\n",
       " ['high', 'precision'],\n",
       " ['hidden', 'layer'],\n",
       " ['language', 'pair'],\n",
       " ['maximum', 'likelihood'],\n",
       " ['probabilistic', 'model'],\n",
       " ['semantic', 'similarity'],\n",
       " ['large', 'corpus'],\n",
       " ['conditional', 'probability'],\n",
       " ['small', 'set'],\n",
       " ['language', 'modeling'],\n",
       " ['translation', 'model'],\n",
       " ['word', 'sequence'],\n",
       " ['probability', 'distribution'],\n",
       " ['overall', 'performance'],\n",
       " ['input', 'sequence'],\n",
       " ['local', 'context'],\n",
       " ['%', 'accuracy'],\n",
       " ['significant', 'improvement'],\n",
       " ['test', 'corpus'],\n",
       " ['data', 'set'],\n",
       " ['vocabulary', 'size'],\n",
       " ['feature', 'space'],\n",
       " ['feature', 'vector'],\n",
       " ['supervised', 'learning'],\n",
       " ['baseline', 'system'],\n",
       " ['classification', 'task'],\n",
       " ['dependency', 'tree'],\n",
       " ['word', 'similarity'],\n",
       " ['dependency', 'relation'],\n",
       " ['gram', 'model'],\n",
       " ['error', 'analysis'],\n",
       " ['automatic', 'evaluation'],\n",
       " ['objective', 'function'],\n",
       " ['correct', 'answer'],\n",
       " ['classification', 'problem'],\n",
       " ['distributional', 'similarity'],\n",
       " ['maximum', 'entropy'],\n",
       " ['high', 'quality'],\n",
       " ['information', 'extraction'],\n",
       " ['vector', 'space'],\n",
       " ['vector', 'representation'],\n",
       " ['learning', 'rate'],\n",
       " ['context', 'vector'],\n",
       " ['learning', 'algorithm'],\n",
       " ['word', 'segmentation'],\n",
       " ['window', 'size'],\n",
       " ['ground', 'truth'],\n",
       " ['context', 'information'],\n",
       " ['translation', 'quality'],\n",
       " ['baseline', 'model'],\n",
       " ['development', 'set'],\n",
       " ['edit', 'distance'],\n",
       " ['_', 'b'],\n",
       " ['machine', 'learning'],\n",
       " ['dialog', 'act'],\n",
       " ['source', 'domain'],\n",
       " ['parallel', 'corpus'],\n",
       " ['word', 'alignment'],\n",
       " ['sentence', 'pair'],\n",
       " ['gold', 'standard'],\n",
       " ['search', 'engine'],\n",
       " ['prior', 'work'],\n",
       " ['beam', 'size'],\n",
       " ['batch', 'size'],\n",
       " ['logistic', 'regression'],\n",
       " ['weight', 'vector'],\n",
       " ['binary', 'classification'],\n",
       " ['coreference', 'resolution'],\n",
       " ['dependency', 'parser'],\n",
       " ['target', 'side'],\n",
       " ['i', 't'],\n",
       " ['entity', 'recognition'],\n",
       " ['active', 'learning'],\n",
       " ['relation', 'extraction'],\n",
       " ['feature', 'selection'],\n",
       " ['validation', 'set'],\n",
       " ['topic', 'model'],\n",
       " ['dependency', 'parsing'],\n",
       " ['target', 'domain'],\n",
       " ['reinforcement', 'learning'],\n",
       " ['cosine', 'similarity'],\n",
       " ['joint', 'model'],\n",
       " ['text', 'classification'],\n",
       " ['human', 'evaluation'],\n",
       " ['art', 'performance'],\n",
       " ['domain', 'adaptation'],\n",
       " ['loss', 'function'],\n",
       " ['test', 'time'],\n",
       " ['tree', 'kernel'],\n",
       " ['hidden', 'state'],\n",
       " ['sentiment', 'classification'],\n",
       " ['sentiment', 'analysis'],\n",
       " ['sequence', 'labeling'],\n",
       " ['phrase', 'pair'],\n",
       " ['h', 't'],\n",
       " ['distant', 'supervision'],\n",
       " ['deep', 'learning'],\n",
       " ['attention', 'mechanism']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "encouraging-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('s3://ling583/micusp.parquet', storage_options={'anon':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "pressed-latest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>micusp/BIO.G0.15.1.html</td>\n",
       "      <td>New York City, 1908: different colors of skin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micusp/BIO.G1.04.1.html</td>\n",
       "      <td>\\tThe fish-tetrapod transition has been calle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>micusp/BIO.G3.03.1.html</td>\n",
       "      <td>\\tIntracellular electric fields are of great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>micusp/BIO.G0.11.1.html</td>\n",
       "      <td>Environmental stresses to plants have been st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>micusp/BIO.G1.01.1.html</td>\n",
       "      <td>\\tThe recurrent cholera pandemics have been re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  filename                                               text\n",
       "0  micusp/BIO.G0.15.1.html   New York City, 1908: different colors of skin...\n",
       "1  micusp/BIO.G1.04.1.html   \\tThe fish-tetrapod transition has been calle...\n",
       "2  micusp/BIO.G3.03.1.html   \\tIntracellular electric fields are of great ...\n",
       "3  micusp/BIO.G0.11.1.html   Environmental stresses to plants have been st...\n",
       "4  micusp/BIO.G1.01.1.html  \\tThe recurrent cholera pandemics have been re..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "likely-payroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "788"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "equivalent-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "restricted-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_core_web_sm is an english model built on data from the web\n",
    "# the \"sm\" denotes small model, there are larger models available but we don't need all of that\n",
    "## Excluded modules:\n",
    "# Parser finds the syntactic structure of sentences\n",
    "# ner (Named Entity Recognizer) pulls out names of people and places\n",
    "# lemmatizer strips imflection and morphology from words to find their root\n",
    "# attribute_ruler identifies gender of pronouns and more\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['parser', 'ner', 'lemmatizer', 'attribute_ruler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "vertical-resident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe57df6a1d0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe57debf310>)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the modules that we are left with after exclusions\n",
    "# took2vec (token to vector) looks words up in the vocabulary\n",
    "# tagger tags the part of speech\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "entitled-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "traditional-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matcher and link it to our vocabulary established above\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# add rules, in this case define candidate terms\n",
    "# IN just means in the set: []\n",
    "# JJ = adjective\n",
    "# NN = noun\n",
    "# IN = preposition\n",
    "# HYPH = hyphen\n",
    "# OP = operation, works like regular expressions (* = zero or more times)\n",
    "matcher.add('Term', [[{'TAG': {'IN': ['JJ', 'NN']}},\n",
    "                      {'TAG': {'IN': ['JJ', 'NN', 'IN', 'HYPH']}, 'OP': '*'},\n",
    "                      {'TAG': 'NN'}]])\n",
    "# this amounts to any noun/adjective followed by and number of adjective/noun/preposition/hyphen, ending with another noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "italic-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = matcher(doc, as_spans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "balanced-inspection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tetrapod', 'transition')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(tok.norm_ for tok in spans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "demographic-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(text):\n",
    "    doc = nlp(text) # tokenize and tag\n",
    "    spans = matcher(doc, as_spans=True) # find all of the spans that satisfy the rules above\n",
    "    return [tuple(tok.norm_ for tok in span) for span in spans] # return a list of all of the spans converted to tuples of normalized strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "sacred-field",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42690ccbcd674e11ac11cf389b650cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/788 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the process on the entire database\n",
    "candidates = list(concat(df['text'].progress_apply(get_candidates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "muslim-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "tested-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict\n",
    "# Keys = sequence lengths\n",
    "# Values = counter of sequences of that length\n",
    "freqs = defaultdict(Counter)\n",
    "for c in candidates:\n",
    "    freqs[len(c)][c] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "indian-constitutional",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "unique-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "beautiful-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of the term, then create a list 1 smaller than that length that decreases until 2\n",
    "def get_subterms(term):\n",
    "    k = len(term)\n",
    "    for m in range(k-1, 1, -1):\n",
    "        yield from ngrams(term, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "decreased-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "toxic-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F = frequency data structure defined above, sorted by length\n",
    "# theta = Threshold, the C-value above which we consider candidates to be terms\n",
    "def c_value(F, theta):\n",
    "    \n",
    "    # Keep track of terms as we identify them\n",
    "    termhood = Counter()\n",
    "    \n",
    "    # Keep track of longer sequences that contain shorter sequences\n",
    "    longer = defaultdict(list)\n",
    "    \n",
    "    # K is sequence length, starting with the longest\n",
    "    for k in sorted(F, reverse=True):\n",
    "        for term in F[k]:\n",
    "            # if the term is a subsequence of a longer one that we have seen already\n",
    "            if term in longer:\n",
    "                discount = sum(longer[term]) / len(longer[term])\n",
    "            # if there are no longer sequences of it, there is no discount\n",
    "            else:\n",
    "                discount = 0\n",
    "            c = log2(k) * (F[k][term] - discount)\n",
    "            if c > theta:\n",
    "                termhood[term] = c\n",
    "                for subterm in get_subterms(term):\n",
    "                    if subterm in F[len(subterm)]:\n",
    "                        longer[subterm].append(F[k][term])\n",
    "    return termhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "buried-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_terms = c_value(freqs, theta=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "humanitarian-inventory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  282.00  282 other hand\n",
      "  264.00  264 health care\n",
      "  252.00  126 part - time faculty\n",
      "  206.00  206 same time\n",
      "  177.52  112 long - term\n",
      "  169.00  169 high school\n",
      "  167.00  167 body color\n",
      "  155.33   98 self - esteem\n",
      "  146.00  146 wing venation\n",
      "  138.00  138 eye color\n",
      "  137.00  137 domestic violence\n",
      "  120.46   76 decision - making\n",
      "  112.53   71 low - income\n",
      "  111.00  111 renewable energy\n",
      "  103.02   65 quality of life\n",
      "  103.02   65 state of nature\n",
      "  103.02   65 spell - caster\n",
      "  103.02   65 community violence exposure\n",
      "  101.00  101 wild type\n",
      "   97.00   97 civil society\n"
     ]
    }
   ],
   "source": [
    "for t, c in new_terms.most_common(20):\n",
    "    print(f'{c:8.2f} {freqs[len(t)][t]:4d} {\" \".join(t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "brief-preserve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   61.00   61 mental health\n",
      "   60.23   86 well - being\n",
      "   57.00   57 child care\n",
      "   57.00   57 egg donation\n",
      "   56.00   28 private for - profit\n",
      "   56.00   28 supra - individual realm\n",
      "   55.47   35 large - scale\n",
      "   55.00   55 storm water\n",
      "   55.00   55 kinetic energy\n",
      "   55.00   55 sex education\n",
      "   54.00   54 grip force\n",
      "   54.00   54 physical activity\n",
      "   54.00  119 community violence\n",
      "   53.89   34 client - provider\n",
      "   53.89   34 recurrent breast cancer\n",
      "   53.00   53 social interaction\n",
      "   53.00   53 professional development\n",
      "   52.30   33 nation - state\n",
      "   52.00   52 wide range\n",
      "   52.00   52 front end\n"
     ]
    }
   ],
   "source": [
    "for t, c in tail(20, new_terms.most_common()):\n",
    "    print(f'{c:8.2f} {freqs[len(t)][t]:4d} {\" \".join(t)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-winning",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Remove non-specific terms**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "passive-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the new terms into a separate file\n",
    "with open('terms2.txt', 'w') as f:\n",
    "    for term in new_terms:\n",
    "        print(' '.join(term), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "precise-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read that same file back in, this converts it from a dict to a list\n",
    "new_terms = [t.split() for t in open('terms2.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "marked-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If loop through the original terms\n",
    "# If the sequence is NOT in the new terms list, add it to a new file\n",
    "with open('terms-final.txt', 'w') as f:\n",
    "    for term in terms:\n",
    "        if term not in new_terms:\n",
    "            print(' '.join(term), file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
